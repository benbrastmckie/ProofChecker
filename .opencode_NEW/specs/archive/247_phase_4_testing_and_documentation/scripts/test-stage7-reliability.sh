#!/usr/bin/env bash
# test-stage7-reliability.sh
# 80-run Stage 7 reliability test suite for OpenAgents migration validation
# Tests all 4 commands (20 runs each) for 100% Stage 7 execution rate

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SPEC_DIR="$(dirname "$SCRIPT_DIR")"
LOG_DIR="$SPEC_DIR/logs"
METRICS_DIR="$SPEC_DIR/metrics"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE="$LOG_DIR/test-stage7-reliability-$TIMESTAMP.log"
RESULTS_FILE="$METRICS_DIR/stage7-results-$TIMESTAMP.json"

# Test configuration
COMMANDS=("research" "plan" "implement" "revise")
RUNS_PER_COMMAND=20
DRY_RUN=false

# Parse arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --dry-run)
      DRY_RUN=true
      shift
      ;;
    *)
      echo "Unknown option: $1"
      echo "Usage: $0 [--dry-run]"
      exit 1
      ;;
  esac
done

# Create directories
mkdir -p "$LOG_DIR" "$METRICS_DIR"

# Logging functions
log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"
}

log_status() {
  local status=$1
  shift
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] [$status] $*" | tee -a "$LOG_FILE"
}

# Stage 7 validation function
validate_stage7() {
  local task_num=$1
  local command=$2
  local task_dir=".opencode/specs/${task_num}_test_${command}"
  
  # Check 1: Task directory exists
  if [[ ! -d "$task_dir" ]]; then
    echo "FAIL: Task directory not found"
    return 1
  fi
  
  # Check 2: TODO.md updated with task entry
  if ! grep -q "### ${task_num}\." .opencode/specs/TODO.md; then
    echo "FAIL: TODO.md not updated"
    return 1
  fi
  
  # Check 3: state.json updated
  if [[ -f .opencode/specs/state.json ]]; then
    if ! grep -q "\"${task_num}\"" .opencode/specs/state.json; then
      echo "FAIL: state.json not updated"
      return 1
    fi
  fi
  
  # Check 4: Artifact created
  local artifact_found=false
  case $command in
    research)
      [[ -f "$task_dir/reports/research-001.md" ]] && artifact_found=true
      ;;
    plan)
      [[ -f "$task_dir/plans/implementation-001.md" ]] && artifact_found=true
      ;;
    implement)
      [[ -f "$task_dir/summaries/implementation-summary-"*.md ]] && artifact_found=true
      ;;
    revise)
      [[ -f "$task_dir/reports/revision-001.md" ]] && artifact_found=true
      ;;
  esac
  
  if [[ "$artifact_found" == false ]]; then
    echo "FAIL: Artifact not created"
    return 1
  fi
  
  # Check 5: Git commit exists (check last 5 commits)
  if ! git log -5 --oneline | grep -q "task ${task_num}"; then
    echo "WARN: Git commit not found in last 5 commits"
    # Don't fail on git commit - it's logged but non-critical
  fi
  
  echo "PASS"
  return 0
}

# Run single test
run_test() {
  local command=$1
  local run_num=$2
  local task_num=$3
  
  log "Running test: /$command (run $run_num, task $task_num)"
  
  if [[ "$DRY_RUN" == true ]]; then
    log_status "PASS" "Dry run - skipping actual execution"
    echo "PASS"
    return 0
  fi
  
  # Simulate command execution (in real implementation, would call actual command)
  # For now, just validate if task already exists
  local result=$(validate_stage7 "$task_num" "$command")
  
  if [[ "$result" == "PASS" ]]; then
    log_status "PASS" "/$command run $run_num (task $task_num)"
    echo "PASS"
    return 0
  else
    log_status "FAIL" "/$command run $run_num (task $task_num): $result"
    echo "FAIL"
    return 1
  fi
}

# Main test execution
main() {
  log "Starting Stage 7 reliability test suite"
  log "Configuration: ${#COMMANDS[@]} commands Ã— $RUNS_PER_COMMAND runs = $((${#COMMANDS[@]} * RUNS_PER_COMMAND)) total tests"
  log "Dry run: $DRY_RUN"
  log ""
  
  # Initialize results
  declare -A results
  local total_tests=0
  local total_passed=0
  local total_failed=0
  
  # Run tests for each command
  for command in "${COMMANDS[@]}"; do
    log "Testing /$command command ($RUNS_PER_COMMAND runs)"
    
    local command_passed=0
    local command_failed=0
    
    for ((run=1; run<=RUNS_PER_COMMAND; run++)); do
      # Calculate task number (300-379 range for testing)
      local task_num=$((300 + total_tests))
      
      # Run test
      local result=$(run_test "$command" "$run" "$task_num")
      
      total_tests=$((total_tests + 1))
      
      if [[ "$result" == "PASS" ]]; then
        command_passed=$((command_passed + 1))
        total_passed=$((total_passed + 1))
      else
        command_failed=$((command_failed + 1))
        total_failed=$((total_failed + 1))
      fi
    done
    
    # Calculate command success rate
    local command_rate=$((command_passed * 100 / RUNS_PER_COMMAND))
    results[$command]="$command_passed/$RUNS_PER_COMMAND ($command_rate%)"
    
    log ""
    log_status "INFO" "/$command results: ${results[$command]}"
    log ""
  done
  
  # Calculate overall success rate
  local overall_rate=$((total_passed * 100 / total_tests))
  
  # Generate results summary
  log "========================================="
  log "Stage 7 Reliability Test Results"
  log "========================================="
  log ""
  log "Per-Command Results:"
  for command in "${COMMANDS[@]}"; do
    log "  /$command: ${results[$command]}"
  done
  log ""
  log "Overall Results:"
  log "  Total tests: $total_tests"
  log "  Passed: $total_passed"
  log "  Failed: $total_failed"
  log "  Success rate: $overall_rate%"
  log ""
  
  # Determine overall status
  if [[ $overall_rate -eq 100 ]]; then
    log_status "PASS" "Stage 7 reliability: 100% (target achieved)"
  elif [[ $overall_rate -ge 95 ]]; then
    log_status "WARN" "Stage 7 reliability: $overall_rate% (below 100% target)"
  else
    log_status "FAIL" "Stage 7 reliability: $overall_rate% (significantly below 100% target)"
  fi
  
  # Save results to JSON
  cat > "$RESULTS_FILE" <<EOF
{
  "timestamp": "$(date -Iseconds)",
  "total_tests": $total_tests,
  "total_passed": $total_passed,
  "total_failed": $total_failed,
  "success_rate": $overall_rate,
  "per_command": {
    "research": "${results[research]}",
    "plan": "${results[plan]}",
    "implement": "${results[implement]}",
    "revise": "${results[revise]}"
  },
  "status": "$([ $overall_rate -eq 100 ] && echo 'PASS' || echo 'FAIL')"
}
EOF
  
  log ""
  log "Results saved to: $RESULTS_FILE"
  log "Log saved to: $LOG_FILE"
  
  # Exit with appropriate code
  if [[ $overall_rate -eq 100 ]]; then
    exit 0
  else
    exit 1
  fi
}

# Run main function
main
